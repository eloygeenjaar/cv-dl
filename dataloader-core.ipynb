{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Union, Dict\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with help from:\n",
    "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\n",
    "class XRayLoader:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 data_path: Path,\n",
    "                 batch_size: int,\n",
    "                 type_of_set: Union['train', 'valid', 'test'],\n",
    "                 pneumonia_dict: Dict[str, int],\n",
    "                 img_shape: Tuple[int, int] = (28, 28),\n",
    "                 augment_rotation: bool = False,\n",
    "                 shuffle: bool = False,\n",
    "                 preload: bool = True):\n",
    "        \n",
    "        \n",
    "        self.data_path: Path = data_path\n",
    "        self.img_shape: Tuple[int, int] = img_shape\n",
    "        self.batch_size: int = batch_size\n",
    "        self.type_of_set: str = type_of_set # e.g. train / valid / test\n",
    "        self.augment_rotation: bool = augment_rotation\n",
    "        self.shuffle: bool = shuffle\n",
    "        self.pneumonia_type_dict: Dict[str, int] = pneumonia_dict\n",
    "        self.healthy_ids : List[int] = []\n",
    "        self.pneumonia_ids: List[int] = []\n",
    "        self.pneumonia_type: List[int] = []\n",
    "        self.healthy_id_regex: str = 'IM-(.*?)-'\n",
    "        self.pneumonia_id_regex: str = 'person(.*?)_'\n",
    "        self.pneumonia_type_regex: str = '_(.*?)_'\n",
    "        self.preload = preload\n",
    "        self.x: np.ndarray = None # (samples, img_shape[0], img_shape[1], channels)\n",
    "            \n",
    "        self.ids: List[int] = []\n",
    "            \n",
    "        self.img_ids: List[int] = []\n",
    "        self.img_names: List[Path] = []\n",
    "        self.current_img_id: int = 0\n",
    "            \n",
    "    def on_epoch_end(self):\n",
    "            'Updates indexes after each epoch'\n",
    "            self.indexes = np.arange(len(self.list_IDs))\n",
    "            if self.shuffle == True:\n",
    "                np.random.shuffle(self.indexes)\n",
    "        \n",
    "    def generate_ids(self):\n",
    "        \n",
    "        self.full_data_path = self.data_path.joinpath(Path(self.type_of_set))\n",
    "        print(f'Finding classes in: {self.full_data_path}')\n",
    "        self.classes = [d for d in self.full_data_path.iterdir() if d.is_dir()]\n",
    "        print(f'Found classes: {[s.name for s in self.classes]}')\n",
    "        for c in self.classes:\n",
    "            files = [f for f in c.iterdir() if f.is_file()]\n",
    "            if c.name.lower() == 'normal':\n",
    "                self.healthy_ids = list(map(self.__extract_healthy_id, files))\n",
    "            elif c.name.lower() == 'pneumonia':\n",
    "                self.pneumonia_ids = list(map(self.__extract_pneumonia_id, files))\n",
    "                self.pneumonia_type = list(map(self.__extract_pneumonia_type, files))\n",
    "            self.img_ids += list(range(self.current_img_id, len(files)))\n",
    "            self.current_img_id += len(files)\n",
    "            self.img_names += [img_name.relative_to(self.data_path) for img_name in c.iterdir() if img_name.is_file()] \n",
    "            if self.preload:\n",
    "                for f in files:\n",
    "                    img_arr = np.asarray(Image.open(f))\n",
    "                    print(img_arr.shape, img_arr.dtype, np.min(img_arr), np.max(img_arr))\n",
    "                    return\n",
    "                    \n",
    "        \n",
    "        self.ids = self.healthy_ids + self.pneumonia_ids\n",
    "        self.targets = [0]*len(self.healthy_ids) + self.pneumonia_type\n",
    "            \n",
    "    def __extract_healthy_id(self, \n",
    "                            p: Path) -> int:\n",
    "        s = p.name\n",
    "        return int(re.search(self.healthy_id_regex, s).group(1))\n",
    "    \n",
    "    def __extract_pneumonia_id(self, \n",
    "                               p: Path) -> int:\n",
    "        s = p.name\n",
    "        return int(re.search(self.pneumonia_id_regex, s).group(1))\n",
    "    \n",
    "    def __extract_pneumonia_type(self, \n",
    "                                 p: Path) -> int:\n",
    "        s = p.name\n",
    "        t = str(re.search(self.pneumonia_type_regex, s).group(1))\n",
    "        return self.pneumonia_type_dict[t]\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding classes in: /Volumes/SEAGATE/chest_xray/train\n",
      "Found classes: ['NORMAL', 'PNEUMONIA']\n",
      "(1858, 2090) uint8 0 255\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "d = {'bacteria': 1, 'virus': 2} # normal = 0\n",
    "x = XRayLoader(Path('/Volumes/SEAGATE/chest_xray'), \n",
    "               batch_size=128, \n",
    "               type_of_set='train',\n",
    "               pneumonia_dict=d)\n",
    "print(x.generate_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = keras.preprocessing.image.ImageDataGenerator(data_format='channels_last')\n",
    "train_generator = datagen.flow_from_directory(\n",
    "                Path('/Volumes/SEAGATE/chest_xray/train'),\n",
    "                color_mode='grayscale',\n",
    "                batch_size=32,\n",
    "                class_mode=None)\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "                Path('/Volumes/SEAGATE/chest_xray/val'),\n",
    "                color_mode='grayscale',\n",
    "                batch_size=16,\n",
    "                class_mode=None)\n",
    "\n",
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)\n",
    "        \n",
    "train_generator = crop_generator(train_generator, 128)\n",
    "validation_generator = crop_generator(validation_generator, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = zip(train_generator, train_generator)\n",
    "vg = zip(validation_generator, validation_generator)\n",
    "\n",
    "class L2Layer(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(L2Layer, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, x):\n",
    "        return tf.keras.backend.l2_normalize(x, axis=0)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_28 (Conv2D)           (None, 34, 34, 64)        1664      \n",
      "_________________________________________________________________\n",
      "lambda_28 (Lambda)           (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 17, 17, 128)       204928    \n",
      "_________________________________________________________________\n",
      "lambda_29 (Lambda)           (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "lambda_30 (Lambda)           (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               2097280   \n",
      "_________________________________________________________________\n",
      "embeddings (Lambda)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16384)             2113536   \n",
      "_________________________________________________________________\n",
      "reshape_10 (Reshape)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "tp1 (Conv2DTranspose)        (None, 17, 17, 128)       295040    \n",
      "_________________________________________________________________\n",
      "tp2 (Conv2DTranspose)        (None, 34, 34, 64)        204864    \n",
      "_________________________________________________________________\n",
      "tp3 (Conv2DTranspose)        (None, 68, 68, 1)         1601      \n",
      "=================================================================\n",
      "Total params: 5,214,081\n",
      "Trainable params: 5,214,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(filters=64, kernel_size=5, strides=2, input_shape=(68, 68, 1), activation='relu', padding='same'))\n",
    "model.add(keras.layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=0)))\n",
    "model.add(keras.layers.Conv2D(filters=128, kernel_size=5, strides=2, activation='relu', padding='same'))\n",
    "model.add(keras.layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=0)))\n",
    "model.add(keras.layers.Conv2D(filters=256, kernel_size=3, strides=2, activation='relu', padding='valid'))\n",
    "model.add(keras.layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=0)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(units=128))\n",
    "model.add(keras.layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=0), name='embeddings'))\n",
    "model.add(keras.layers.Dense(units=16384))\n",
    "#model.add(keras.layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=0)))\n",
    "model.add(keras.layers.Reshape( (8, 8, 256) ))\n",
    "model.add(keras.layers.Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='valid', activation='relu', name='tp1'))\n",
    "#model.add(keras.layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=0)))\n",
    "model.add(keras.layers.Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same', activation='relu', name='tp2'))\n",
    "#model.add(keras.layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=0)))\n",
    "model.add(keras.layers.Conv2DTranspose(filters=1, kernel_size=5, strides=2, padding='same', activation='relu', name='tp3'))\n",
    "model.compile(optimizer='adam',\n",
    "                  loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "163/163 [==============================] - 525s 3s/step - loss: 3604.3970 - val_loss: 4085.9221\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 496s 3s/step - loss: 1595.5620 - val_loss: 3521.9709\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 493s 3s/step - loss: 1573.7879 - val_loss: 3741.1333\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 494s 3s/step - loss: 1564.2683 - val_loss: 3797.3054\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 494s 3s/step - loss: 1562.0074 - val_loss: 4005.0369\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 492s 3s/step - loss: 1543.5827 - val_loss: 3060.7495\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 494s 3s/step - loss: 1555.1748 - val_loss: 3498.7854\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 485s 3s/step - loss: 1574.2945 - val_loss: 2514.1504\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 474s 3s/step - loss: 1538.2700 - val_loss: 3625.6648\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 474s 3s/step - loss: 1540.1167 - val_loss: 4419.5044\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 474s 3s/step - loss: 1545.2840 - val_loss: 3349.6160\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 478s 3s/step - loss: 1539.7253 - val_loss: 3328.7085\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 480s 3s/step - loss: 1545.1771 - val_loss: 3677.3835\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 478s 3s/step - loss: 1528.7676 - val_loss: 2826.5083\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 481s 3s/step - loss: 1532.7846 - val_loss: 3303.7642\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 500s 3s/step - loss: 1552.2763 - val_loss: 2760.8450\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 484s 3s/step - loss: 1526.1154 - val_loss: 3163.1848\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 480s 3s/step - loss: 1531.5388 - val_loss: 3708.9512\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 481s 3s/step - loss: 1531.1326 - val_loss: 4134.7729\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 480s 3s/step - loss: 1533.5792 - val_loss: 2271.6221\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 674s 4s/step - loss: 1542.4879 - val_loss: 2918.6643\n",
      "Epoch 22/50\n",
      " 57/163 [=========>....................] - ETA: 7:58 - loss: 1520.2553"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 6] Device not configured",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.7/site-packages/keras/utils/data_utils.py\", line 650, in next_sample\n    return six.next(_SHARED_SEQUENCES[uid])\n  File \"/usr/local/lib/python3.7/site-packages/keras_preprocessing/image/iterator.py\", line 100, in __next__\n    return self.next(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/keras_preprocessing/image/iterator.py\", line 112, in next\n    return self._get_batches_of_transformed_samples(index_array)\n  File \"/usr/local/lib/python3.7/site-packages/keras_preprocessing/image/iterator.py\", line 226, in _get_batches_of_transformed_samples\n    interpolation=self.interpolation)\n  File \"/usr/local/lib/python3.7/site-packages/keras_preprocessing/image/utils.py\", line 126, in load_img\n    img = img.resize(width_height_tuple, resample)\n  File \"/usr/local/lib/python3.7/site-packages/PIL/Image.py\", line 1817, in resize\n    self.load()\n  File \"/usr/local/lib/python3.7/site-packages/PIL/ImageFile.py\", line 224, in load\n    s = read(self.decodermaxblock)\n  File \"/usr/local/lib/python3.7/site-packages/PIL/JpegImagePlugin.py\", line 397, in load_read\n    s = self.fp.read(read_bytes)\nOSError: [Errno 6] Device not configured\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ccd5f615c478>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m          \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m          \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m          shuffle=True)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    740\u001b[0m                     \u001b[0;34m\"`use_multiprocessing=False, workers > 1`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                     \"For more information see issue #1638.\")\n\u001b[0;32m--> 742\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.7/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 6] Device not configured"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "         generator=tg,  \n",
    "         epochs=50,\n",
    "         steps_per_epoch=163,\n",
    "         validation_data = vg,\n",
    "         validation_steps=1,\n",
    "         verbose=1,\n",
    "         validation_freq=1,\n",
    "         use_multiprocessing=True,\n",
    "         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = datagen.flow_from_directory(\n",
    "                Path('/Volumes/SEAGATE/chest_xray/test'),\n",
    "                color_mode='grayscale',\n",
    "                target_size=(28, 28),\n",
    "                batch_size=16,\n",
    "                class_mode='binary')\n",
    "encoder = keras.Model(inputs=model.input, outputs=model.get_layer(name='lambda_8').output)\n",
    "embeddings = encoder.predict_generator(train_generator)\n",
    "\n",
    "print(f'Embeddings shape: {embeddings.shape}')\n",
    "clusters = KMeans(n_clusters=3).fit_transform(embeddings)\n",
    "print(f'Clusters shape: {clusters.shape}')\n",
    "\n",
    "def plot_tsne(data: np.ndarray,\n",
    "              labels: np.ndarray,\n",
    "              dataset_name: np.ndarray,\n",
    "              num_samples_per_class: int = 100) -> None:\n",
    "    unique_labels = np.unique(labels)\n",
    "    x_tsne = np.array([])\n",
    "    for i in range(len(unique_labels)):\n",
    "        label_ix = np.argwhere(unique_labels[i] == labels).flatten()\n",
    "        random_labels_ix = np.random.choice(label_ix, num_samples_per_class)\n",
    "        random_label_data = data[random_labels_ix, :]\n",
    "        print(np.mean(random_label_data), np.std(random_label_data))\n",
    "        random_label_data = np.reshape(random_label_data, (num_samples_per_class, -1))\n",
    "        if i == 0:\n",
    "            x_tsne = random_label_data\n",
    "        else:\n",
    "            x_tsne = np.vstack((x_tsne, random_label_data))\n",
    "    tsne_emb = TSNE(n_components=2).fit_transform(x_tsne)\n",
    "    i = 0\n",
    "    for i in range(len(unique_labels)):\n",
    "        rgb = np.random.rand(3,)\n",
    "        start_slice = (i)*num_samples_per_class\n",
    "        stop_slice = (i+1)*num_samples_per_class\n",
    "        x_plot = tsne_emb[()]\n",
    "        if i == 0:\n",
    "            plt.scatter(tsne_emb[start_slice:stop_slice, 0], tsne_emb[start_slice:stop_slice,1], c='b', s=10, alpha=0.5)\n",
    "        else:\n",
    "            plt.scatter(tsne_emb[start_slice:stop_slice, 0], tsne_emb[start_slice:stop_slice,1], c='r', s=10, alpha=0.5)\n",
    "    plt.title(f'T-SNE features {dataset_name}')\n",
    "    plt.legend(unique_labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(embeddings,\n",
    "          test_generator.classes,\n",
    "          'Xray dataset',\n",
    "          num_samples_per_class=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(clusters,\n",
    "          test_generator.classes,\n",
    "          'Xray dataset',\n",
    "          num_samples_per_class=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
